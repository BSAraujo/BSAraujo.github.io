---
title: "Support Vector Machines with the Hard-Margin Loss: Optimal Training via Combinatorial Benders' Cuts"
collection: talks
type: "Workshop"
permalink: /talks/2023-04-28-berlin
venue: "&quot;Exploring synergies: Machine Learning meets Physics and Optimization&quot;, Thematic Einstein Semester on Mathematical Optimization for Machine Learning"
date: 2023-04-28
location: "Zuse Institute Berlin, Germany"
---

[More information here](https://mathplus.de/topic-development-lab/tes-summer-2023/workshop/)

The classical support vector machine (SVM) model is sensitive to outlier observations due to its unbounded loss function. To circumvent this issue, recent studies focused on the hard-margin loss, which provides robustness but leads to an NP-hard model, challenging current exact optimization algorithms. Against this background, we propose new integer programming strategies that significantly improve our ability to train the hard-margin SVM to global optimality. We introduce an iterative sampling and decomposition approach, in which smaller subproblems are used to separate combinatorial Bendersâ€™ cuts. Those cuts, used within a branch-and-cut algorithm, speed up convergence towards a global optimum. Our algorithm solves, for the first time, 117 classical benchmark datasets to optimality and achieves a reduction of 50% in the average optimality gap for the hardest benchmark datasets.

